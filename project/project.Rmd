---
title: "STA 631 Project"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output:
  github_document: default
  pdf_document: 
    latex_engine: xelatex
    dev: jpeg
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  cache.lazy = FALSE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 180,
  fig.width = 8,
  fig.height = 5
)
library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
library(knitr)
library(furniture)
theme_set(theme_minimal())
```

# Load data into R and preprocess

```{r}
bike <- read.csv("day.csv", stringsAsFactors = FALSE)

bike$weekday <-
  factor(
    bike$weekday,
    levels = 0:6,
    labels = c('SUN', 'MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT')
  )
bike$holiday <-
  factor(bike$holiday,
         levels = c(0, 1),
         labels = c('NO HOLIDAY', 'HOLIDAY'))
bike$workingday <-
  factor(
    bike$workingday,
    levels = c(0, 1),
    labels = c('NO WORKING DAY', 'WORKING DAY')
  )
bike$season <-
  factor(
    bike$season,
    levels = 1:4,
    labels = c('SPRING', 'SUMMER', 'FALL', 'WINTER')
  )
bike$weathersit <-
  factor(
    bike$weathersit,
    levels = 1:3,
    labels = c('GOOD', 'MISTY', 'RAIN/SNOW/STORM')
  )
bike$mnth <-
  factor(
    bike$mnth,
    levels = 1:12,
    labels = c(
      'JAN',
      'FEB',
      'MAR',
      'APR',
      'MAY',
      'JUN',
      'JUL',
      'AUG',
      'SEP',
      'OCT',
      'NOV',
      'DEC'
    )
  )
bike$yr[bike$yr == 0] <- 2011
bike$yr[bike$yr == 1] <- 2012
bike$yr <- factor(bike$yr)

bike$temp <-  bike$temp * (39 - (-8)) + (-8)
bike$atemp <- bike$atemp * (50 - (16)) + (16)
bike$windspeed  <-  67 * bike$windspeed
bike$hum <-  100 * bike$hum

bike <- bike %>% select(-dteday)
```

# Summary and removal of three variables  

```{r}
bike_summary <- skim(bike)
bike_summary

table_summary <- table1(bike, output = 'markdown')
table_summary

# check if "cnt" = "casual" + "registered"  
sum(bike$cnt) == sum(bike$casual) + sum(bike$registered)

bike <- bike %>%
  select(-c(instant, casual, registered, yr))
```

# Visualization


```{r}
bike_numeric_hist <- bike %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x = value, fill = key), color = "black") +
  facet_wrap( ~ key, scales = "free")
bike_numeric_hist

bike_numeric_scatter <- bike %>%
  keep(is.numeric) %>%
  pivot_longer(-cnt, names_to = "Feature", values_to = "Value") %>%
  ggplot() +
  geom_point(mapping = aes(x = Value, y = cnt, color = Feature)) +
  geom_smooth(mapping = aes(x = Value, y = cnt), method = 'lm') +
  facet_wrap( ~ Feature, scales = "free", ncol = 3) +
  scale_x_continuous(n.breaks = 2) +
  theme(legend.position = "",
        plot.title.position = "plot") +
  labs(x = "Numeric Feature Value",
       title = "Bike Rental Numeric Variables versus Rental Count")
bike_numeric_scatter

numeric_cor <- bike %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::rearrange() %>%
  corrr::shave() %>%
  corrr::rplot(shape = 15,
               colors = c("darkorange", "white", "darkcyan"))
numeric_cor


bike %>%
  select(-atemp) %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::network_plot(min_cor = 0.2)

bike %>%
  keep(is.numeric) %>%
  pivot_longer(everything()) %>%
  ggplot() +
  aes(x = value) +
  geom_density() +
  facet_wrap( ~ name, scales = "free")

bike %>%
  keep(is.numeric) %>%
  cor() %>%
  as_tibble(rownames = "x") %>%
  pivot_longer(-x) %>%
  ggplot() +
  aes(x = x, y = name, fill = value) +
  geom_raster() +
  scale_fill_gradient2(low = "purple", mid = "white",
                       high = "orangered") +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(
    angle = 90,
    hjust = 1,
    vjust = 0.5
  ))
```

```{r}
bike %>% 
  keep(is.factor) %>% 
  ggpairs()

bike_factors <- bike %>%
  keep(is.factor) %>% 
  colnames()

chart <- c(bike_factors,"cnt")

bike %>%
  select_at(vars(chart)) %>%
  pivot_longer(-cnt, names_to = "Factor", values_to = "Level") %>%
  ggplot() +
  geom_boxplot(mapping = aes(x = Level, y = cnt, fill = Factor)) +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap( ~ Factor, scales = "free", ncol = 3) +
  theme(legend.position = "",
        plot.title.position = "plot") +
  labs(x = "Categorical Feature Value",
       title = "Bike Rental Categorical Variable versus Rental Count")

observed_indep_statistic <- bike %>%
  specify(season ~ mnth) %>%
  calculate(stat = "Chisq")  

null_distribution_simulated <- bike %>%
  specify(season ~ mnth) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 2000, type = "permute") %>%
  calculate(stat = "Chisq")

null_distribution_theoretical <- bike %>%
  specify(season ~ mnth) %>%
  hypothesize(null = "independence") %>%
  calculate(stat = "Chisq")

null_distribution_simulated %>%
  visualize() + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")

bike %>%
  specify(season ~ mnth) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")

null_distribution_simulated %>%
  visualize(method = "both") + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")

p_value_independence <- null_distribution_simulated %>%
  get_p_value(obs_stat = observed_indep_statistic,
              direction = "greater")
p_value_independence

chisq_test(bike, season ~ mnth)
```

# Modeling  

```{r}
set.seed(1)
bike_data <- bike %>% 
  select(-atemp)

set.seed(1)
data_split <- initial_split(bike_data)
train_split <- training(data_split)
test_split  <- testing(data_split)

train_fold <- vfold_cv(train_split)

model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
```

## base line 

```{r}
mean_rmse <- sqrt(mean((mean(train_split$cnt) - test_split$cnt)^2))
mean_rmse

mean_pred <- rep(mean(train_split$cnt), nrow(test_split))
mean_rmse_1 <-  rmse(test_split, cnt, mean_pred)
mean_rmse_1
```

## OLS  

```{r}
glm_mod <- lm(cnt ~ ., data = train_split)
glm_pred <- predict(glm_mod, test_split)
glm_rmse <- rmse(test_split, cnt, glm_pred)
glm_rmse
glm_rsq <- rsq(test_split, cnt, glm_pred)
glm_rsq
glm_coef <- coef(glm_mod)
```

## best subset  

```{r}
library(leaps)

best_subset_mod <-
  regsubsets(cnt ~ ., data = train_split, nvmax = 27)

reg_summary <- summary(best_subset_mod)

reg_summary

plot(reg_summary$rsq,
     xlab = "Number of Variables",
     ylab = "R2",
     type = "l")
r2_max <- which.max(reg_summary$rsq)
points(r2_max,
       reg_summary$rsq[r2_max],
       col = "red",
       cex = 2,
       pch = 20)

#par(mfrow = c(2,2))
plot(reg_summary$rss,
     xlab = "Number of Variables",
     ylab = "RSS",
     type = "l")
rss_min <- which.min(reg_summary$rss)
points(
  rss_min,
  reg_summary$rss[rss_min],
  col = "red",
  cex = 2,
  pch = 20
)

plot(reg_summary$adjr2,
     xlab = "Number of Variables",
     ylab = "Adjusted RSq",
     type = "l")
adj_r2_max <- which.max(reg_summary$adjr2)
points(
  adj_r2_max,
  reg_summary$adjr2[adj_r2_max],
  col = "red",
  cex = 2,
  pch = 20
)

plot(reg_summary$cp,
     xlab = "Number of Variables",
     ylab = "Cp",
     type = "l")
cp_min <- which.min(reg_summary$cp)
points(cp_min,
       reg_summary$cp[cp_min],
       col = "red",
       cex = 2,
       pch = 20)

plot(reg_summary$bic,
     xlab = "Number of Variables",
     ylab = "BIC",
     type = "l")
bic_min <- which.min(reg_summary$bic)
points(
  bic_min,
  reg_summary$bic[bic_min],
  col = "red",
  cex = 2,
  pch = 20
)


test_mat <- model.matrix (cnt ~ ., data = test_split)

val_errors <- rep(NA, ncol(train_split) - 1)

for (i in 1:(ncol(train_split) - 1)) {
  coefi <- coef(best_subset_mod, id = i)
  pred <- test_mat[, names(coefi)] %*% coefi
  val_errors[i] <-  mean((test_split$cnt - pred) ^ 2)
}
min <- which.min(val_errors)
plot(val_errors, type = 'b')
points(min,
       val_errors[min][1],
       col = "red",
       cex = 2,
       pch = 20)

sub_best <- regsubsets(cnt ~ ., data = train_split, nvmax = min)
coef(sub_best, min)
best_sub_RMSE <- sqrt(val_errors[min])
best_sub_RMSE

best_sub_rsq <- reg_summary$adjr2[min]
best_sub_rsq

min
```

## best subset with CV

```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}

k <- 10
set.seed(1)

folds <- sample(1:k, nrow(train_split), replace = TRUE)

cv_errors <- matrix(NA, k, 27, dimnames = list(NULL, paste(1:27)))

for (j in 1:k) {
  best_fit <-
    regsubsets(cnt ~ ., data = train_split[folds != j, ], nvmax = 27)
  for (i in 1:26) {
    pred <- predict(best_fit, train_split[folds == j, ], id = i)
    cv_errors[j, i] <- mean((train_split$cnt[folds == j] - pred) ^ 2)
  }
}

mean_cv_errors <- apply(cv_errors, 2, mean)

min <- which.min(mean_cv_errors)

plot(mean_cv_errors, type = 'b')
points(min,
       mean_cv_errors[min][1],
       col = "red",
       cex = 2,
       pch = 20)

reg_best <- regsubsets(cnt ~ ., data = train_split, nvmax = min)
coef(reg_best, min)
best_sub_cv_RMSE <- sqrt(mean_cv_errors[min])
best_sub_cv_RMSE
```

## ridge  

```{r}
library(glmnet)
set.seed(1)

x_train <- model.matrix(cnt ~ ., train_split)[, -1]
x_test <- model.matrix(cnt ~ ., test_split)[, -1]
x_full <- model.matrix(cnt ~ ., bike_data)[, -1]

y_train <- train_split %>%
  select(cnt) %>%
  unlist()

y_test <- test_split %>%
  select(cnt) %>%
  unlist()

y_full <-  bike_data %>% 
  select(cnt) %>%
  unlist()

grid <- 10 ^ seq(10, -2, length = 100)

ridge_mod <- glmnet(x_train,
                    y_train,
                    alpha = 0,
                    lambda = grid,
                    thresh = 1e-12)

cv.out <- cv.glmnet(x_train, y_train, alpha = 0)
bestlam <- cv.out$lambda.min

ridge_pred <- predict(ridge_mod, s = bestlam, newx = x_test)
ridge_RMSE <- sqrt(mean((ridge_pred - y_test) ^ 2)) 
ridge_RMSE

out <- glmnet(x_full, y_full, alpha = 0, lambda = grid)
ridge_coef <- predict(out, type = "coefficients", s = bestlam)[1:27,]
ridge_coef
```

## LASSO 

```{r}
lasso_mod <- glmnet(x_train,
                    y_train,
                    alpha = 1,
                    lambda = grid)
set.seed(1)
cv.out <- cv.glmnet(x_train, y_train, alpha = 1)
bestlam <- cv.out$lambda.min
lasso_pred <- predict(lasso_mod, s = bestlam, newx = x_test)
lasso_RMSE <- sqrt(mean((lasso_pred - y_test) ^ 2))
lasso_RMSE

out <- glmnet(x_full, y_full, alpha = 1, lambda = grid)
lasso_coef <- predict(out, type = "coefficients", s = bestlam)[1:27,]
lasso_coef[lasso_coef != 0]
```

## PCR  

```{r}
library(pls)

set.seed(1)
pcr_fit <- pcr(cnt ~ .,
               data = train_split,
               scale = TRUE,
               validation = "CV")

summary(pcr_fit)

validationplot(pcr_fit, val.type = "MSEP")

pcr_fit2 <- pcr(y_train ~ x_train, scale = TRUE, ncomp = 20)
summary(pcr_fit2)

pcr_pred <- predict(pcr_fit2, x_test, ncomp = 20)

pcr_RMSE <- sqrt(mean((pcr_pred - y_test) ^ 2))
pcr_RMSE
```

## PLS  

```{r}
set.seed(1)
pls_fit <- plsr(cnt ~ .,
               data = train_split,
               scale = TRUE,
               validation = "CV")
summary(pls_fit)
validationplot(pls_fit, val.type = "MSEP")
pls_pred <- predict(pls_fit, x_test, ncomp = 5)
pls_RMSE <- sqrt(mean((pls_pred - y_test) ^ 2))
pls_RMSE
pls_fit1 <- plsr(cnt ~ .,
                 data = bike_data,
                 scale = TRUE,
                 ncomp = 5)
summary(pls_fit1)
```

## tidymodels - LASSO

```{r}
glmnet_recipe <-
  recipe(formula = cnt ~ ., data = train_split) %>% 
  step_dummy(all_nominal()) %>% 
  step_normalize(all_numeric(), -all_outcomes())


glmnet_spec <-
  linear_reg(penalty = tune(),
             mixture = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

glmnet_workflow <-
  workflow() %>%
  add_recipe(glmnet_recipe) %>%
  add_model(glmnet_spec)

glmnet_grid <-
  tidyr::crossing(
    penalty = 10 ^ seq(-6,-1, length.out = 20),
    mixture = c(0.05,
                0.2, 0.4, 0.6, 0.8, 1)
  )

doParallel::registerDoParallel()

set.seed(1)
glmnet_tune <-
  tune_grid(glmnet_workflow, 
            resamples = train_fold, 
            grid = glmnet_grid) 

doParallel::stopImplicitCluster()

glmnet_final <-
  glmnet_workflow %>%
  finalize_workflow(select_best(glmnet_tune))

glmnet_fit <- 
  last_fit(glmnet_final, data_split)

glmnet_fit %>%
  collect_metrics()

glmnet_terms <- fit(glmnet_final, train_split) %>%
  pull_workflow_fit() %>%
  tidy()

autoplot(glmnet_tune)

glmnet_tune %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

collect_predictions(glmnet_fit) %>%
  ggplot(aes(cnt, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()
```

## tidymodels - KNN

```{r}
knn_rec <-
  recipe(formula = cnt ~ ., data = train_split) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal()) %>%
  prep()

knn_spec <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>%
  set_engine("kknn") %>%
  set_mode("regression")

knn_workflow <- workflow() %>% 
  add_recipe(knn_rec) %>% 
  add_model(knn_spec)

knn_grid <- grid_regular(neighbors(),
                         weight_func(),
                         dist_power(),
                         levels = 2)

doParallel::registerDoParallel()

set.seed(1)
knn_rs <- tune_grid(knn_workflow,
                   resamples = train_fold,
                   grid = knn_grid,
                   control = model_control)

doParallel::stopImplicitCluster()

final_knn <- 
  knn_workflow %>% 
  finalize_workflow(select_best(knn_rs, "rmse"))

set.seed(1)
fit_knn <- 
  last_fit(final_knn, data_split)

fit_knn %>%
  collect_metrics()
```

## tidymodels - Random Forest

```{r}
ranger_recipe <-
  recipe(formula = cnt ~ ., data = train_split)

ranger_spec <-
  rand_forest(mtry = tune(),
              min_n = tune(),
              trees = 1000) %>%
  set_mode("regression") %>%
  set_engine("ranger")

ranger_workflow <-
  workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(ranger_spec)

doParallel::registerDoParallel()

set.seed(1)
ranger_tune <- tune_grid(ranger_workflow,
                         resamples = train_fold,
                         grid = 20)

doParallel::stopImplicitCluster()

ranger_final <- 
  ranger_workflow %>% 
  finalize_workflow(select_best(ranger_tune))

ranger_fit <- 
  last_fit(ranger_final, data_split)

ranger_fit %>%
  collect_metrics()

autoplot(ranger_tune)

collect_predictions(ranger_fit) %>%
  ggplot(aes(cnt, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()

ranger_imp_spec <- ranger_spec %>%
  finalize_model(select_best(ranger_tune)) %>%
  set_engine("ranger", importance = "permutation")

workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(ranger_imp_spec) %>%
  fit(train_split) %>%
  pull_workflow_fit() %>%
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue"))

fit(ranger_final, train_split) %>%
  pull_workflow_fit()
```

## tidymodels - Decision Tree

```{r}
tree_rec <- 
  recipe(cnt ~ ., train_split)

tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_workflow <- workflow() %>% 
  add_recipe(tree_rec) %>% 
  add_model(tree_spec)

tree_grid <- tree_spec %>% 
  parameters() %>% 
  grid_regular(levels = 4)

doParallel::registerDoParallel()

set.seed(1)
tree_rs <- tune_grid(
  tree_workflow,
  resamples = train_fold,
  grid = tree_grid,
  control = model_control
)

final_tree <- 
  tree_workflow %>% 
  finalize_workflow(select_best(tree_rs, "rmse"))

fit_tree <- 
  last_fit(final_tree, data_split)

doParallel::stopImplicitCluster()

fit_tree %>%
  collect_metrics()
```

```{r}
tidy_data <- recipe(cnt~., data = train_split) %>% 
  step_dummy(all_nominal()) %>% 
  prep() %>% 
  juice
```

# compare RMSE

```{r}
rmses <- c(pull(glm_rmse), best_sub_RMSE, best_sub_cv_RMSE, ridge_RMSE, lasso_RMSE, pcr_RMSE, pls_RMSE)

models <- c("OLS","best subset", "best subset with CV", "ridge", "lasso", "PCR", "PLS")

comparison <- data.frame(models, rmses)
comparison %>% kable()

coef_compare <- cbind(cbind(glm_coef, ridge_coef), lasso_coef)

collect_metrics(glmnet_fit) %>%
  bind_rows(collect_metrics(fit_knn)) %>%
  bind_rows(collect_metrics(ranger_fit)) %>%
  bind_rows(collect_metrics(fit_tree)) %>%
  filter(.metric == "rmse") %>%
  mutate(model = c("lasso_t", "knn_t", "rf_t", "dtree_t")) %>%
  knitr::kable()
```

## OLS with interaction

```{r}
glm_mod_interaction <- lm(cnt ~ .+season*mnth, data = train_split)
glm_pred_interaction <- predict(glm_mod_interaction, test_split)
glm_rmse_i <- rmse(test_split, cnt, glm_pred_interaction)
glm_rmse_i
glm_rsq_i <- rsq(test_split, cnt, glm_pred_interaction)
glm_rsq_i
glm_coef_i <- coef(glm_mod_interaction)
```

