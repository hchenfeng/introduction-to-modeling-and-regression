---
title: "STA 631 Project"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output:
  github_document: default
  pdf_document: 
    latex_engine: xelatex
    dev: jpeg
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  cache.lazy = FALSE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 180,
  fig.width = 8,
  fig.height = 5
)
library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
library(vip)
library(knitr)
library(furniture)
theme_set(theme_minimal())
```

# Load data into R and preprocess

```{r}
bike <- read.csv("day.csv", stringsAsFactors = FALSE)

bike$weekday <-
  factor(
    bike$weekday,
    levels = 0:6,
    labels = c('SUN', 'MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT')
  )
bike$holiday <-
  factor(bike$holiday,
         levels = c(0, 1),
         labels = c('NO HOLIDAY', 'HOLIDAY'))
bike$workingday <-
  factor(
    bike$workingday,
    levels = c(0, 1),
    labels = c('NO WORKING DAY', 'WORKING DAY')
  )
bike$season <-
  factor(
    bike$season,
    levels = 1:4,
    labels = c('SPRING', 'SUMMER', 'FALL', 'WINTER')
  )
bike$weathersit <-
  factor(
    bike$weathersit,
    levels = 1:3,
    labels = c('GOOD', 'MISTY', 'RAIN/SNOW/STORM')
  )
bike$mnth <-
  factor(
    bike$mnth,
    levels = 1:12,
    labels = c(
      'JAN',
      'FEB',
      'MAR',
      'APR',
      'MAY',
      'JUN',
      'JUL',
      'AUG',
      'SEP',
      'OCT',
      'NOV',
      'DEC'
    )
  )
bike$yr[bike$yr == 0] <- 2011
bike$yr[bike$yr == 1] <- 2012
bike$yr <- factor(bike$yr)

bike$temp <-  bike$temp * (39 - (-8)) + (-8)
bike$atemp <- bike$atemp * (50 - (16)) + (16)
bike$windspeed  <-  67 * bike$windspeed
bike$hum <-  100 * bike$hum

day_diff <- function(date1, date2) {
  as.numeric(difftime(as.Date(date1), as.Date(date2), units = 'days'))
}

bike$days_since_2011 <- day_diff(bike$dteday, min(as.Date(bike$dteday)))

bike <- bike %>% select(-dteday)
```

# Summary and removal of three variables  

```{r}
bike_summary <- skim(bike)
bike_summary

table_summary <- table1(bike, output = 'markdown')
table_summary

# check if "cnt" = "casual" + "registered"  
sum(bike$cnt) == sum(bike$casual) + sum(bike$registered)

bike <- bike %>%
  select(-c(instant, casual, registered))
```

# Visualization

#```{r}
#bike_numeric_plot <- bike %>%
#  select(temp:days_since_2011) %>%
#  pivot_longer(c(temp:windspeed, days_since_2011)) %>%
#  ggplot(aes(cnt, value)) +
#  geom_point() +
#  geom_smooth(method = "lm") +
#  facet_wrap( ~ name, scales = "free_y")
#bike_numeric_plot
#
#bike_categorical_plot <- bike %>%
#  select(c(season:weathersit, cnt)) %>%
#  pivot_longer(season:weathersit) %>%
#  ggplot(aes(cnt, value)) +
#  geom_boxplot() +
#  facet_wrap( ~ name, scales = "free_y")
#bike_categorical_plot
#```

```{r}
bike_numeric_hist <- bike %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x = value, fill = key), color = "black") +
  facet_wrap( ~ key, scales = "free")
bike_numeric_hist

bike_numeric_scatter <- bike %>%
  keep(is.numeric) %>%
  pivot_longer(-cnt, names_to = "Feature", values_to = "Value") %>%
  ggplot() +
  geom_point(mapping = aes(x = Value, y = cnt, color = Feature)) +
  geom_smooth(mapping = aes(x = Value, y = cnt), method = 'lm') +
  facet_wrap( ~ Feature, scales = "free", ncol = 3) +
  scale_x_continuous(n.breaks = 2) +
  theme(legend.position = "",
        plot.title.position = "plot") +
  labs(x = "Numeric Feature Value",
       title = "Bike Rental Numeric Variables versus Rental Count")
bike_numeric_scatter

numeric_cor <- bike %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::rearrange() %>%
  corrr::shave() %>%
  corrr::rplot(shape = 15,
               colors = c("darkorange", "white", "darkcyan"))
numeric_cor


bike %>%
  select(-atemp) %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::network_plot(min_cor = 0.2)

bike %>%
  keep(is.numeric) %>%
  pivot_longer(everything()) %>%
  ggplot() +
  aes(x = value) +
  geom_density() +
  facet_wrap( ~ name, scales = "free")

bike %>%
  keep(is.numeric) %>%
  cor() %>%
  as_tibble(rownames = "x") %>%
  pivot_longer(-x) %>%
  ggplot() +
  aes(x = x, y = name, fill = value) +
  geom_raster() +
  scale_fill_gradient2(low = "purple", mid = "white",
                       high = "orangered") +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(
    angle = 90,
    hjust = 1,
    vjust = 0.5
  ))
```

```{r}
bike %>% 
  keep(is.factor) %>% 
  ggpairs()

bike_factors <- bike %>%
  keep(is.factor) %>% 
  colnames()

chart <- c(bike_factors,"cnt")

bike %>%
  select_at(vars(chart)) %>%
  pivot_longer(-cnt, names_to = "Factor", values_to = "Level") %>%
  ggplot() +
  geom_boxplot(mapping = aes(x = Level, y = cnt, fill = Factor)) +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap( ~ Factor, scales = "free", ncol = 4) +
  theme(legend.position = "",
        plot.title.position = "plot") +
  labs(x = "Categorical Feature Value",
       title = "Bike Rental Categorical Variable versus Rental Count")

observed_indep_statistic <- bike %>%
  specify(season ~ mnth) %>%
  calculate(stat = "Chisq")  

null_distribution_simulated <- bike %>%
  specify(season ~ mnth) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 5000, type = "permute") %>%
  calculate(stat = "Chisq")

null_distribution_theoretical <- bike %>%
  specify(season ~ mnth) %>%
  hypothesize(null = "independence") %>%
  calculate(stat = "Chisq")

null_distribution_simulated %>%
  visualize() + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")

bike %>%
  specify(season ~ mnth) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")

null_distribution_simulated %>%
  visualize(method = "both") + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")

p_value_independence <- null_distribution_simulated %>%
  get_p_value(obs_stat = observed_indep_statistic,
              direction = "greater")
p_value_independence

chisq_test(bike, season ~ mnth)
```

# Modeling  

```{r}
set.seed(1)
bike_data <- bike %>%
  dplyr::select(-c(atemp))

set.seed(1)
data_split <- initial_split(bike_data)
train_split <- training(data_split)
test_split  <- testing(data_split)

train_fold <- vfold_cv(train_split)

model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
```

## base line 

```{r}
mean_rmse <- sqrt(mean((mean(train_split$cnt) - test_split$cnt)^2))
mean_rmse

mean_pred <- rep(mean(train_split$cnt), nrow(test_split))
mean_rmse_1 <-  rmse(test_split, cnt, mean_pred)
mean_rmse_1
```

```{r}
glm_mod <- lm(cnt ~ ., data = train_split)
glm_pred <- predict(glm_mod, test_split)
glm_rmse <- rmse(test_split, cnt, glm_pred)
glm_rmse
glm_rsq <- rsq(test_split, cnt, glm_pred)
glm_rsq

par(mfrow = c(2, 2))
plot(glm_mod)
doParallel::registerDoParallel()
stan_glm_mod <- stan_glm(cnt ~., data = tidy_rec, refresh = 0)
stan_glm_pred <- predict(stan_glm_mod, se.fit = FALSE, newdata = test_split)
doParallel::stopImplicitCluster()
stan_glm_rmse <- rmse(test_split, cnt, stan_glm_pred)
stan_glm_rmse
```

```{r}
pca_rec <- recipe(cnt ~ ., data = train_split) %>%
  step_corr(all_numeric(),-all_outcomes()) %>%
  step_lincomb(all_numeric(),-all_outcomes()) %>%
  step_normalize(all_numeric(),-all_outcomes()) %>%
  step_dummy(all_nominal()) %>%
  step_pca(all_predictors(), num_comp = 5)

pca_regression_model <- linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

pca_wf <- workflow() %>% 
  add_model(pca_regression_model) %>% 
  add_recipe(pca_rec)

doParallel::registerDoParallel()

pca_res <- fit_resamples(
  pca_wf,
  resamples = train_fold,
  control = model_control
)

doParallel::registerDoParallel()

pca_res %>% 
  collect_metrics()
```
```{r}
spline_rec <- recipe(cnt~., data = train_split) %>% 
  step_corr(all_numeric(), -all_outcomes()) %>% 
  step_lincomb(all_numeric(), -all_outcomes()) %>% 
  step_rm(all_nominal()) %>% 
  step_bs(all_predictors()) %>% 
  step_YeoJohnson(all_predictors())

spline_model <- linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

spline_wf <- workflow() %>% 
  add_model(spline_model) %>% 
  add_recipe(spline_rec)

doParallel::registerDoParallel()

spline_res <- fit_resamples(
  spline_wf,
  resamples = train_fold,
  control = model_control
)

doParallel::stopImplicitCluster()

spline_res %>% 
  collect_metrics()
```
```{r}
tidy_rec <- recipe(cnt~., data = train_split) %>% 
  step_dummy(all_nominal()) %>% 
  prep %>% juice
```


```{r}
glmnet_recipe <-
  recipe(formula = cnt ~ ., data = train_split)

glmnet_spec <-
  linear_reg(penalty = tune(),
             mixture = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

glmnet_workflow <-
  workflow() %>%
  add_recipe(glmnet_recipe) %>%
  add_model(glmnet_spec)

glmnet_grid <-
  tidyr::crossing(
    penalty = 10 ^ seq(-6,-1, length.out = 20),
    mixture = c(0.05,
                0.2, 0.4, 0.6, 0.8, 1)
  )

doParallel::registerDoParallel()

set.seed(1)
glmnet_tune <-
  tune_grid(glmnet_workflow, 
            resamples = train_fold, 
            grid = glmnet_grid) 

doParallel::stopImplicitCluster()

glmnet_final <-
  glmnet_workflow %>%
  finalize_workflow(select_best(glmnet_tune))

glmnet_fit <- 
  last_fit(glmnet_final, data_split)

glmnet_fit %>%
  collect_metrics()

glmnet_terms <- fit(glmnet_final, train_split) %>%
  pull_workflow_fit() %>%
  tidy()

autoplot(glmnet_tune)

glmnet_tune %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

collect_predictions(glmnet_fit) %>%
  ggplot(aes(cnt, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()

glmnet_imp_spec <- glmnet_spec %>%
  finalize_model(select_best(glmnet_tune)) %>%
  set_engine("glmnet", importance = "rmse")

variable_importance <- workflow() %>%
  add_recipe(glmnet_recipe) %>%
  add_model(glmnet_imp_spec) %>%
  fit(train_split) %>%
  pull_workflow_fit() %>%
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue"))
```

```{r}
knn_rec <-
  recipe(formula = cnt ~ ., data = train_split) %>% 
  step_normalize(all_predictors(), -all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal()) %>% 
  prep()

knn_spec <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>%
  set_engine("kknn") %>%
  set_mode("regression")

knn_workflow <- workflow() %>% 
  add_recipe(knn_rec) %>% 
  add_model(knn_spec)

knn_grid <- grid_regular(neighbors(),
                         weight_func(),
                         dist_power(),
                         levels = 2)

doParallel::registerDoParallel()

set.seed(1)
knn_rs <- tune_grid(knn_workflow,
                   resamples = train_fold,
                   grid = knn_grid,
                   control = model_control)

doParallel::stopImplicitCluster()

final_knn <- 
  knn_workflow %>% 
  finalize_workflow(select_best(knn_rs, "rmse"))

set.seed(1)
fit_knn <- 
  last_fit(final_knn, data_split)

fit_knn %>%
  collect_metrics()
```

```{r}
ranger_recipe <-
  recipe(formula = cnt ~ ., data = train_split)
#
ranger_spec <-
  rand_forest(mtry = tune(),
              min_n = tune(),
              trees = 1000) %>%
  set_mode("regression") %>%
  set_engine("ranger")
#
ranger_workflow <-
  workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(ranger_spec)
#
doParallel::registerDoParallel()

set.seed(1)
ranger_tune <- tune_grid(ranger_workflow,
                         resamples = train_fold,
                         grid = 20)
#
doParallel::stopImplicitCluster()
#
ranger_final <- 
  ranger_workflow %>% 
  finalize_workflow(select_best(ranger_tune))
#
ranger_fit <- 
  last_fit(ranger_final, data_split)
#
ranger_fit %>%
  collect_metrics()
#
autoplot(ranger_tune)
#
collect_predictions(ranger_fit) %>%
  ggplot(aes(cnt, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()
#
ranger_imp_spec <- ranger_spec %>%
  finalize_model(select_best(ranger_tune)) %>%
  set_engine("ranger", importance = "permutation")
#
workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(ranger_imp_spec) %>%
  fit(train_split) %>%
  pull_workflow_fit() %>%
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue"))

fit(ranger_final, train_split) %>%
  pull_workflow_fit()
```

```{r}
tree_rec <- 
  recipe(cnt ~ ., train_split)

tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_workflow <- workflow() %>% 
  add_recipe(tree_rec) %>% 
  add_model(tree_spec)

tree_grid <- tree_spec %>% 
  parameters() %>% 
  grid_regular(levels = 4)

doParallel::registerDoParallel()

set.seed(1)
tree_rs <- tune_grid(
  tree_workflow,
  resamples = train_fold,
  grid = tree_grid,
  control = model_control
)

final_tree <- 
  tree_workflow %>% 
  finalize_workflow(select_best(tree_rs, "rmse"))

fit_tree <- 
  last_fit(final_tree, data_split)

doParallel::stopImplicitCluster()

fit_tree %>%
  collect_metrics()
```


