---
title: "STA 631 Project Report"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    dev: jpeg
  github_document: default
  html_document:
    fig_caption: yes
    theme: lumen
    toc: yes
    toc_depth: 2
    df_print: kable
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  cache.lazy = FALSE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi=180,
  fig.width = 8,
  fig.height = 5
)
```
```{r eval=FALSE, include=FALSE}
renderpdfreport <- function() {
  rmarkdown::render("report.Rmd", output_format = "pdf_document")
}
```

# Introduction

As a toy problem, we are investigating two years' daily bike rental data from Capital Bikeshare and the weather condition during the period, in order to see if we could make reliable predictions on how many bikes are rented given some information about the weather and the day. To be more specific, the statistical question we ask is this: is there a linear relationship between our predictor variables (or a subset of them) and the number of daily bike rentals? If there is, predictions from such a relationship could help the bike rental company determine how to adjust their resources to better meet rental bike needs. 

The data we use here is archived in the UCI machine learning repository and have been used in research projects (Fanaee-T & Gama, 2014) and demonstrations (Molnar, 2020). Our main interest is to practice applying some of the methods we learned about multiple linear regression to the dataset. 

***  

# Methods

The data we use here have been processed and contain no missing values or obvious outliers. We do some preprocessing following Molnar (https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R). We convert season, year, month, holiday, weekday, working day, and weather situation into factors and assign to them corresponding labels. We also change temperature, feeling temperature, wind speed, and humidity back to the original scale (the dataset contains normalized values for these variables). We also drop the instant variable, as we do not take it to be relevant for our analysis.

Here is a brief overview of all predictors and the outcome for the dataset after the processing. 


|                 | description |Mean/Count (SD/%) |
|-----------------|-------------|------------------|
|                 |             |     n = 731      |
|     season      | seasons of the year||
|     SPRING      | |   181 (24.8%)    |
|     SUMMER      | |   184 (25.2%)    |
|      FALL       | |   188 (25.7%)    |
|     WINTER      | |   178 (24.4%)    |
|      mnth       | month ||
|       JAN       | |    62 (8.5%)     |
|       FEB       | |    57 (7.8%)     |
|       MAR       | |    62 (8.5%)     |
|       APR       | |    60 (8.2%)     |
|       MAY       | |    62 (8.5%)     |
|       JUN       | |    60 (8.2%)     |
|       JUL       | |    62 (8.5%)     |
|       AUG       | |    62 (8.5%)     |
|       SEP       | |    60 (8.2%)     |
|       OCT       | |    62 (8.5%)     |
|       NOV       | |    60 (8.2%)     |
|       DEC       | |    62 (8.5%)     |
|     holiday     | holiday or not ||
|   NO HOLIDAY    | |   710 (97.1%)    |
|     HOLIDAY     | |    21 (2.9%)     |
|     weekday     | day of the week ||
|       SUN       | |   105 (14.4%)    |
|       MON       | |   105 (14.4%)    |
|       TUE       | |   104 (14.2%)    |
|       WED       | |   104 (14.2%)    |
|       THU       | |   104 (14.2%)    |
|       FRI       | |   104 (14.2%)    |
|       SAT       | |   105 (14.4%)    |
|   workingday    | working day or not ||
| NO WORKING DAY  | |   231 (31.6%)    |
|   WORKING DAY   | |   500 (68.4%)    |
|   weathersit    | weather situation ||
|      GOOD       | |   463 (63.3%)    |
|      MISTY      | |   247 (33.8%)    |
| RAIN/SNOW/STORM | |    21 (2.9%)     |
|      temp       | temperature in Celcius | 15.3 (8.6) |
|      atemp      | feel-like temperature in Celcius| 32.1 (5.5) |
|       hum       | humidity |  62.8 (14.2) |
|    windspeed    | wind speed in km/hr | 12.8 (5.2) |
|     casual      | count of bikes rented by casual users | 848.2 (686.6) |
|   registered    | count of bikes rented by registered users | 3656.2 (1560.3) |
|       cnt       | count of bikes rented | 4504.3 (1937.2) |


The outcome variable is "cnt", daily count of bike rented. All the rest are candidate predictor variables. There are a few variables we can drop based on our knowledge of the dataset. First, we can confirm that "casual" and "registered" are partitions of the total count. Since our focus here is on the daily number of bike rentals, we can drop both "casual" and "registered". We also drop the year variable because we want to build models beyond the years of our data. We should expect dependence between between "temp" and "atemp", as they are supposed to be related. Month and season is related, so is weekday and working day. Temperature, humidity, and wind speed may be related to season/month in some way. We wait until we have done some preliminary analysis before deciding how to deal with multicolinearity.

We use ggplot2 for most of our plots. As an exercise, we practice modeling with both the "glm" package from "stats" and the "tidymodels" package. "Tidymodels" is a collection of packages which tries to streamline many of the steps in common data analysis tasks. It follows the "tidyverse" way in terms of syntax and data structure and makes the modeling process more consistent. "Tidymodels" makes it relatively easy to incorporate resampling into the model building process and tune hyperparameters.   

***  

#	Results

## Exploratory Data Analysis  

First, we check the relationship between numerical variables and the outcome.

```{r}
bike %>%
  keep(is.numeric) %>%
  pivot_longer(-cnt, names_to = "Feature", values_to = "Value") %>%
  ggplot() +
  geom_point(mapping = aes(x = Value, y = cnt, color = Feature)) +
  geom_smooth(mapping = aes(x = Value, y = cnt), method = 'lm') +
  facet_wrap( ~ Feature, scales = "free", ncol = 2) +
  scale_x_continuous(n.breaks = 2) +
  theme(legend.position = "",
        plot.title.position = "plot") +
  labs(x = "Numeric Feature Value",
       title = "Bike Rental Numeric Variables versus Rental Count")
```
We see positive linear relationships between variables "atemp" and "temp" on one side, and bike rental counts on the other. We also see negative linear relationships between humidity and wind speed on the one side, and the number of bike rentals on the other. However, as we look at the pairwise correlation of the numerical variables, including the outcome, we can confirm the correlation between "temp" and "atemp" (Pearson correlation coefficient `r cor(bike$temp, bike$atemp)`). 

```{r }
bike %>%
  keep(is.numeric) %>%
  cor() %>%
  as_tibble(rownames = "x") %>%
  pivot_longer(-x) %>%
  ggplot() +
  aes(x = x, y = name, fill = value) +
  geom_raster() +
  scale_fill_gradient2(low = "purple", mid = "white",
                       high = "orangered") +
  labs(x = NULL, y = NULL)
```

This is expected, because feel-like temperature is a linear transformation of the real temperature. We choose to use "temp" in our model and drop "atemp". We see correlations between other numerical variables, but none rise to above 0.8 as to concern us.    

We take a look at the relationships between the categorical variables and the outcome.  

```{r}
bike %>%
  select_at(vars(chart)) %>%
  pivot_longer(-cnt, names_to = "Factor", values_to = "Level") %>%
  ggplot() +
  geom_boxplot(mapping = aes(x = Level, y = cnt, fill = Factor)) +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap( ~ Factor, scales = "free", ncol = 3) +
  theme(legend.position = "",
        plot.title.position = "plot") +
  labs(x = "Categorical Feature Value",
       title = "Bike Rental Categorical Variable versus Rental Count")
```

We see that the median number of bike rentals drop during holidays. There seems to be more rentals during the warmer months/seasons. And understandably, more people rent bikes during days of good weather. 

As said before, we expect month to be correlated with season, working day correlated with weekday. There may also be correlations between season/month and temperature/humidity/wind speed. It is a question, however, what kind of problem these correlations may pose to our model. It is hard to quantify all the correlations on the same scale. Since we care more about prediction, we keep them all in the models for now.

One other thing we can confirm is that the outcome variable is approximately normal. 

```{r}
bike %>% 
  select(cnt) %>% 
  ggplot(aes(cnt)) + 
  geom_histogram()
```

It does not look perfect, but the assumption of normality is met to a large extent.

## Modeling  

We establish a base line using only the mean. We find the rmse estimate is 1863. 

We fit a linear model using all current predictors (6 factor variables and 4 numerical variables) and obtain an rmse of 1231 and rsq of 0.562. We want to try some other models and see if we can improve the rmse. 

Out of the 28 predictor variables (convert factors to indicators), using best subset leave us with 8 variables and best subset with cross validation 18. However, from both we get higher rmses than from the OLS model. 

We see that most coefficients are shrinked in the ridge and lasso models when compared with OLS (see a subset in the following). 

|           | glm_coef| ridge_coef| lasso_coef|
|:----------|--------:|----------:|----------:|
|weekdayTUE | 322.7308|   34.91181|   0.000000|
|weekdayWED | 436.4934|  109.17161|   2.326309|
|weekdayTHU | 297.3481|   82.80112|   0.000000|
|weekdayFRI | 345.9217|  118.42933|  11.062837|
|weekdaySAT | 488.3060|  354.69077| 245.762542|

Having tried a number of other models, here is a brief summary of their rmses:  

|model               | rmse.   |
|:-------------------|--------:|
|mean                | 1863.   |
|OLS                 | 1231.145|
|best subset         | 1247.642|
|best subset with CV | 1318.826|
|ridge               | 1241.488|
|lasso               | 1240.179|
|PCR                 | 1290.540|
|PLS                 | 1242.930|
|lasso_t             | 1231.697|
|knn_t               | 1289.344|
|random_forest_t     | 1203.405|
|decision_tree_t     | 1341.265|

We see that the OLS model performs better than most other models. The lasso model we trained using the tuning feature provided in "tidymodels" achieves similar performance, but that model only drops one level of the mnth variable from the set of variables. Interestingly, the random forest model (tuned) returns the lowest rmse.  

If our focus is solely on prediction, then the random forest model is our top choice. The added bonus of using random forest is that we need to worry less about assumptions on the data. However, it is hard to interpret random forest models. Having put some more thought into the question we are trying to solve, we think interpretability matters in this context.

Our original aim was to predict the number of bike rentals based on the features we have. But it seems unnecessary to build a model to predict the maximum number of bikes needed for a region. To acquire that number, the easiest way would be to get the maximum number of bikes rented over the period. The features in our predictor set do not seem to vary a lot by year. A prediction focused model based on this feature set would not have much utility. However, with a reasonably performant linear model, we may be able to understand the effect of different features on the number of bike rentals. Later on, if we find it necessary to include additional features to our model, we may actually be able to compare the effects of different features.

We take a look at the diagnostic plots for the OLS model.  

```{r}
par(mfrow = c(2, 2))
plot(glm_mod)
```

The residual vs fitted and the scale-location plots are both approximately random. There is a slight curve in the Q-Q plot, but it does show an overall linear pattern. None of the high leverage points in the residuals vs leverage plot should worry us too much. In general, it looks like the OLS model works reasonably well for our data. We can confirm this from the pred vs. actual plot for our test data.  

```{r}
plot(glm_pred, test_split$cnt)
```

We can see the overall linear pattern clearly, although the width of the spread around the line may be somewhat wide. We may try adding some interaction to our model or using spline.  


***  

#	Conclusions

We set out to find a best model for predicting the number of bike rentals on the basis of available features. We practiced fitting a few different models, even attempted some using "tidymodels". It has been a good learning experience. The surprising finding here is that the questions for data analysis does not need to be fixed at the beginning. Iterating through the steps of analyzing datasets may end up help us clarify the questions we actually care. 

***  

# References

Fanaee-T, H., & Gama, J. (2014). Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence, 2(2-3), 113-127.

Molnar, C. (2020). Interpretable Machine Learning. United States: Lulu.com.

https://christophm.github.io/interpretable-ml-book/limo.html

http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset

https://www.tidymodels.org/learn/

https://juliasilge.com/

https://en.wikipedia.org/wiki/Heat_index



#	Appendix

Here is a list of files submitted with the report:  

•	proposal

•	rmarkdown for analysis  

•	rmarkdown for analysis pdf output  